{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob as glob\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection import RetinaNet_ResNet50_FPN_V2_Weights\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 4  # Increase / decrease according to GPU memeory.\n",
    "RESIZE_TO = 640  # Resize the image for training and transforms.\n",
    "NUM_EPOCHS = 40  # Number of epochs to train for.\n",
    "NUM_WORKERS = 4  # Number of parallel workers for data loading.\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Training images and labels files directory.\n",
    "TRAIN_DIR = \"data/train\"\n",
    "# Validation images and labels files directory.\n",
    "VALID_DIR = \"data/valid\"\n",
    "\n",
    "# Classes: 0 index is reserved for background.\n",
    "CLASSES = [\n",
    "    \"__background__\",\n",
    "    \"elbow positive\",\n",
    "    \"fingers positive\",\n",
    "    \"forearm fracture\",\n",
    "    \"humerus fracture\",\n",
    "    \"humerus\",\n",
    "    \"shoulder fracture\",\n",
    "    \"wrist positive\",\n",
    "]\n",
    "\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "# Whether to visualize images after crearing the data loaders.\n",
    "VISUALIZE_TRANSFORMED_IMAGES = True\n",
    "\n",
    "# Location to save model and plots.\n",
    "OUT_DIR = \"outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averager:\n",
    "    \"\"\"\n",
    "    A class to keep track of running average of values (e.g. training loss).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "\n",
    "class SaveBestModel:\n",
    "    \"\"\"\n",
    "    Saves the model if the current epoch's validation mAP is higher\n",
    "    than all previously observed values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, best_valid_map=float(0)):\n",
    "        self.best_valid_map = best_valid_map\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        model,\n",
    "        current_valid_map,\n",
    "        epoch,\n",
    "        OUT_DIR,\n",
    "    ):\n",
    "        if current_valid_map > self.best_valid_map:\n",
    "            self.best_valid_map = current_valid_map\n",
    "            print(f\"\\nBEST VALIDATION mAP: {self.best_valid_map}\")\n",
    "            print(f\"SAVING BEST MODEL FOR EPOCH: {epoch+1}\\n\")\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                },\n",
    "                f\"{OUT_DIR}/best_model.pth\",\n",
    "            )\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    To handle the data loading as different images may have different\n",
    "    numbers of objects, and to handle varying-size tensors as well.\n",
    "    \"\"\"\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "def get_train_transform():\n",
    "    # We keep \"pascal_voc\" because bounding box format is [x_min, y_min, x_max, y_max].\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        bbox_params={\"format\": \"pascal_voc\", \"label_fields\": [\"labels\"]},\n",
    "    )\n",
    "\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        bbox_params={\"format\": \"pascal_voc\", \"label_fields\": [\"labels\"]},\n",
    "    )\n",
    "\n",
    "\n",
    "def show_tranformed_image(train_loader):\n",
    "    \"\"\"\n",
    "    Visualize transformed images from the `train_loader` for debugging.\n",
    "    Only runs if `VISUALIZE_TRANSFORMED_IMAGES = True` in config.py.\n",
    "    \"\"\"\n",
    "    if len(train_loader) > 0:\n",
    "        for i in range(BATCH_SIZE):\n",
    "            images, targets = next(iter(train_loader))\n",
    "            images = list(image.to(DEVICE) for image in images)\n",
    "\n",
    "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "            for i in range(len(images)):\n",
    "                if len(targets[i][\"boxes\"]) == 0:\n",
    "                    continue\n",
    "                boxes = targets[i][\"boxes\"].cpu().numpy().astype(np.int32)\n",
    "                labels = targets[i][\"labels\"].cpu().numpy().astype(np.int32)\n",
    "                sample = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "                sample = cv2.cvtColor(sample, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                for box_num, box in enumerate(boxes):\n",
    "                    cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), (0, 0, 255), 2)\n",
    "                    cv2.putText(\n",
    "                        sample,\n",
    "                        CLASSES[labels[box_num]],\n",
    "                        (box[0], box[1] - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        1.0,\n",
    "                        (0, 0, 255),\n",
    "                        2,\n",
    "                    )\n",
    "                cv2.imshow(\"Transformed image\", sample)\n",
    "                cv2.waitKey(0)\n",
    "                cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def save_model(epoch, model, optimizer):\n",
    "    \"\"\"\n",
    "    Save the trained model (state dict) and optimizer state to disk.\n",
    "    \"\"\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        },\n",
    "        \"outputs/last_model.pth\",\n",
    "    )\n",
    "\n",
    "\n",
    "def save_loss_plot(OUT_DIR, train_loss_list, x_label=\"iterations\", y_label=\"train loss\", save_name=\"train_loss\"):\n",
    "    \"\"\"\n",
    "    Saves the training loss curve.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(train_loss_list, color=\"tab:blue\")\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.savefig(f\"{OUT_DIR}/{save_name}.png\")\n",
    "    print(\"SAVING PLOTS COMPLETE...\")\n",
    "\n",
    "\n",
    "def save_mAP(OUT_DIR, map_05, map):\n",
    "    \"\"\"\n",
    "    Saves the mAP@0.5 and mAP@0.5:0.95 curves per epoch.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(map_05, color=\"tab:orange\", linestyle=\"-\", label=\"mAP@0.5\")\n",
    "    plt.plot(map, color=\"tab:red\", linestyle=\"-\", label=\"mAP@0.5:0.95\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"mAP\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{OUT_DIR}/map.png\")\n",
    "    print(\"SAVING mAP PLOTS COMPLETE...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dir_path, width, height, classes, transforms=None):\n",
    "        \"\"\"\n",
    "        :param dir_path: Directory with 'images/' and 'labels/' sub-folders.\n",
    "        :param width: Resized width for images.\n",
    "        :param height: Resized height for images.\n",
    "        :param classes: List of class names.\n",
    "        :param transforms: Albumentations transforms to apply.\n",
    "        \"\"\"\n",
    "        self.transforms = transforms\n",
    "        self.dir_path = dir_path\n",
    "        self.image_dir = os.path.join(self.dir_path, \"images\")\n",
    "        self.label_dir = os.path.join(self.dir_path, \"labels\")\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.classes = classes\n",
    "\n",
    "        self.image_file_types = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.ppm\", \"*.JPG\"]\n",
    "        self.all_image_paths = []\n",
    "        for file_type in self.image_file_types:\n",
    "            self.all_image_paths.extend(glob.glob(os.path.join(self.image_dir, file_type)))\n",
    "\n",
    "        # Sort the paths so that images and labels stay in consistent order.\n",
    "        self.all_image_paths = sorted(self.all_image_paths)\n",
    "        self.all_image_names = [os.path.basename(img_p) for img_p in self.all_image_paths]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.all_image_names[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        label_filename = os.path.splitext(image_name)[0] + \".txt\"\n",
    "        label_path = os.path.join(self.label_dir, label_filename)\n",
    "\n",
    "        # Read and preprocess image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        orig_h, orig_w = image.shape[:2]\n",
    "\n",
    "        # Resize the image to (self.width, self.height)\n",
    "        image_resized = cv2.resize(image, (self.width, self.height))\n",
    "        image_resized /= 255.0  # scale to [0,1]\n",
    "\n",
    "        # Read the normalized bounding boxes from the .txt file\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                # Each line: class_id x_min y_min x_max y_max (normalized)\n",
    "                parts = line.split()\n",
    "                class_id = parts[0]\n",
    "                xmin = float(parts[1])\n",
    "                ymin = float(parts[2])\n",
    "                xmax = float(parts[3])\n",
    "                ymax = float(parts[4])\n",
    "\n",
    "                # Convert class_name to label index\n",
    "                label_idx = int(class_id) + 1\n",
    "\n",
    "                # Because bounding boxes are normalized [0..1],\n",
    "                # directly multiply by self.width, self.height:\n",
    "                x_min_final = xmin * self.width\n",
    "                x_max_final = xmax * self.width\n",
    "                y_min_final = ymin * self.height\n",
    "                y_max_final = ymax * self.height\n",
    "\n",
    "                # Ensure max coords > min coords\n",
    "                if x_max_final == x_min_final:\n",
    "                    x_max_final += 1\n",
    "                if y_max_final == y_min_final:\n",
    "                    y_max_final += 1\n",
    "\n",
    "                # Clip coords if they exceed boundaries after scaling\n",
    "                x_min_final = max(0, min(x_min_final, self.width - 1))\n",
    "                x_max_final = max(0, min(x_max_final, self.width))\n",
    "                y_min_final = max(0, min(y_min_final, self.height - 1))\n",
    "                y_max_final = max(0, min(y_max_final, self.height))\n",
    "\n",
    "                boxes.append([x_min_final, y_min_final, x_max_final, y_max_final])\n",
    "                labels.append(label_idx)\n",
    "\n",
    "        # Convert to tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        area = (\n",
    "            (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "            if len(boxes) > 0\n",
    "            else torch.tensor([], dtype=torch.float32)\n",
    "        )\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        target[\"image_id\"] = image_id\n",
    "\n",
    "        # Apply transforms if any\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(image=image_resized, bboxes=target[\"boxes\"], labels=labels)\n",
    "            image_resized = sample[\"image\"]\n",
    "            # 'sample[\"bboxes\"]' can be a list of tuples; convert to tensor\n",
    "            target[\"boxes\"] = torch.tensor(sample[\"bboxes\"], dtype=torch.float32)\n",
    "\n",
    "        # If no boxes or NaNs, fix that scenario\n",
    "        if target[\"boxes\"].numel() == 0:\n",
    "            target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "\n",
    "        return image_resized, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_image_paths)\n",
    "\n",
    "\n",
    "# Create dataset & loader\n",
    "def create_train_dataset(DIR):\n",
    "    train_dataset = CustomDataset(\n",
    "        dir_path=DIR, width=RESIZE_TO, height=RESIZE_TO, classes=CLASSES, transforms=get_train_transform()\n",
    "    )\n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "def create_valid_dataset(DIR):\n",
    "    valid_dataset = CustomDataset(\n",
    "        dir_path=DIR, width=RESIZE_TO, height=RESIZE_TO, classes=CLASSES, transforms=get_valid_transform()\n",
    "    )\n",
    "    return valid_dataset\n",
    "\n",
    "\n",
    "def create_train_loader(train_dataset, num_workers=0):\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "def create_valid_loader(valid_dataset, num_workers=0):\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    return valid_loader\n",
    "\n",
    "\n",
    "# Quick test if this file is run directly\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = CustomDataset(TRAIN_DIR, RESIZE_TO, RESIZE_TO, CLASSES)\n",
    "    print(f\"Number of training images: {len(dataset)}\")\n",
    "\n",
    "    def visualize_sample(image, target):\n",
    "        # Convert to NumPy for OpenCV visualization\n",
    "        # img = image.transpose(1, 2, 0)  # shape: (H, W, C)\n",
    "        img = image\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        boxes = target[\"boxes\"].cpu().numpy().astype(np.int32)\n",
    "        labels = target[\"labels\"].cpu().numpy().astype(np.int32)\n",
    "        for box_num, box in enumerate(boxes):\n",
    "            cv2.rectangle(img, (box[0], box[1]), (box[2], box[3]), (0, 0, 255), 2)\n",
    "            class_str = CLASSES[labels[box_num]]\n",
    "            cv2.putText(img, class_str, (box[0], box[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        cv2.imshow(\"Sample\", img)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "    NUM_SAMPLES_TO_VISUALIZE = 3\n",
    "    for i in range(NUM_SAMPLES_TO_VISUALIZE):\n",
    "        image, target = dataset[i]\n",
    "        visualize_sample(image, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes=91):\n",
    "    \"\"\"\n",
    "    Creates a RetinaNet-ResNet50-FPN v2 model pre-trained on COCO.\n",
    "    Replaces the classification head for the required number of classes.\n",
    "    \"\"\"\n",
    "    model = torchvision.models.detection.retinanet_resnet50_fpn_v2(weights=RetinaNet_ResNet50_FPN_V2_Weights.COCO_V1)\n",
    "    num_anchors = model.head.classification_head.num_anchors\n",
    "\n",
    "    # Replace the classification head\n",
    "    model.head.classification_head = RetinaNetClassificationHead(\n",
    "        in_channels=256, num_anchors=num_anchors, num_classes=num_classes, norm_layer=partial(torch.nn.GroupNorm, 32)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = create_model(num_classes=8)\n",
    "    print(model)\n",
    "    # Total parameters:\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{total_params:,} total parameters.\")\n",
    "    # Trainable parameters:\n",
    "    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"{total_trainable_params:,} training parameters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# Function for running training iterations.\n",
    "def train(train_data_loader, model):\n",
    "    print(\"Training\")\n",
    "    model.train()\n",
    "\n",
    "    # initialize tqdm progress bar\n",
    "    prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n",
    "\n",
    "    for i, data in enumerate(prog_bar):\n",
    "        optimizer.zero_grad()\n",
    "        images, targets = data\n",
    "\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        train_loss_hist.send(loss_value)\n",
    "\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "    return loss_value\n",
    "\n",
    "\n",
    "# Function for running validation iterations.\n",
    "def validate(valid_data_loader, model):\n",
    "    print(\"Validating\")\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize tqdm progress bar.\n",
    "    prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n",
    "    target = []\n",
    "    preds = []\n",
    "    for i, data in enumerate(prog_bar):\n",
    "        images, targets = data\n",
    "\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images, targets)\n",
    "\n",
    "        # For mAP calculation using Torchmetrics.\n",
    "        #####################################\n",
    "        for i in range(len(images)):\n",
    "            true_dict = dict()\n",
    "            preds_dict = dict()\n",
    "            true_dict[\"boxes\"] = targets[i][\"boxes\"].detach().cpu()\n",
    "            true_dict[\"labels\"] = targets[i][\"labels\"].detach().cpu()\n",
    "            preds_dict[\"boxes\"] = outputs[i][\"boxes\"].detach().cpu()\n",
    "            preds_dict[\"scores\"] = outputs[i][\"scores\"].detach().cpu()\n",
    "            preds_dict[\"labels\"] = outputs[i][\"labels\"].detach().cpu()\n",
    "            preds.append(preds_dict)\n",
    "            target.append(true_dict)\n",
    "        #####################################\n",
    "\n",
    "    metric.reset()\n",
    "    metric.update(preds, target)\n",
    "    metric_summary = metric.compute()\n",
    "    return metric_summary\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    train_dataset = create_train_dataset(TRAIN_DIR)\n",
    "    valid_dataset = create_valid_dataset(VALID_DIR)\n",
    "    train_loader = create_train_loader(train_dataset, NUM_WORKERS)\n",
    "    valid_loader = create_valid_loader(valid_dataset, NUM_WORKERS)\n",
    "    print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "    print(f\"Number of validation samples: {len(valid_dataset)}\\n\")\n",
    "\n",
    "    # Initialize the model and move to the computation device.\n",
    "    model = create_model(num_classes=NUM_CLASSES)\n",
    "    model = model.to(DEVICE)\n",
    "    print(model)\n",
    "    # Total parameters and trainable parameters.\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{total_params:,} total parameters.\")\n",
    "    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"{total_trainable_params:,} training parameters.\")\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.002, momentum=0.9, nesterov=True)\n",
    "    scheduler = StepLR(optimizer=optimizer, step_size=15, gamma=0.1)\n",
    "\n",
    "    # To monitor training loss\n",
    "    train_loss_hist = Averager()\n",
    "    # To store training loss and mAP values.\n",
    "    train_loss_list = []\n",
    "    map_50_list = []\n",
    "    map_list = []\n",
    "\n",
    "    # Mame to save the trained model with.\n",
    "    MODEL_NAME = \"model\"\n",
    "\n",
    "    # Whether to show transformed images from data loader or not.\n",
    "    if VISUALIZE_TRANSFORMED_IMAGES:\n",
    "        from custom_utils import show_tranformed_image\n",
    "\n",
    "        show_tranformed_image(train_loader)\n",
    "\n",
    "    # To save best model.\n",
    "    save_best_model = SaveBestModel()\n",
    "\n",
    "    metric = MeanAveragePrecision()\n",
    "\n",
    "    # Training loop.\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n",
    "\n",
    "        # Reset the training loss histories for the current epoch.\n",
    "        train_loss_hist.reset()\n",
    "\n",
    "        # Start timer and carry out training and validation.\n",
    "        start = time.time()\n",
    "        train_loss = train(train_loader, model)\n",
    "        metric_summary = validate(valid_loader, model)\n",
    "        print(f\"Epoch #{epoch+1} train loss: {train_loss_hist.value:.3f}\")\n",
    "        print(f\"Epoch #{epoch+1} mAP: {metric_summary['map']:.3f}\")\n",
    "        end = time.time()\n",
    "        print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch}\")\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "        map_50_list.append(metric_summary[\"map_50\"])\n",
    "        map_list.append(metric_summary[\"map\"])\n",
    "\n",
    "        # save the best model till now.\n",
    "        save_best_model(model, float(metric_summary[\"map\"]), epoch, \"outputs\")\n",
    "        # Save the current epoch model.\n",
    "        save_model(epoch, model, optimizer)\n",
    "\n",
    "        # Save loss plot.\n",
    "        save_loss_plot(OUT_DIR, train_loss_list)\n",
    "\n",
    "        # Save mAP plot.\n",
    "        save_mAP(OUT_DIR, map_50_list, map_list)\n",
    "        scheduler.step()\n",
    "        print(\"Current LR:\", scheduler.get_last_lr())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
